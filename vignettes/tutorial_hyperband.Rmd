---
title: "hyperbandr tutorial"
author: "by Niklas"
output: 
  pdf_document: 
    keep_tex: yes
header-includes:
  - \usepackage{graphicx}
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# hyperbandr

This is an R6 implementation of the original **hyperband** algorithm  <https://arxiv.org/abs/1603.06560>.

R6 is an encapsulated object oriented system akin to those in Java or C++, where objects contain methods in addition to data, and those methods can modify objects directly (unlike S3 and S4 which are both functional object-oriented systems, where class methods are separate from objects, and objects are not mutable).

Essentially, that means that we obtain a very generic implementation, which is working with every other R package (as long the algorithm meets the requirements of hyperband).

### This tutorial contains a general introduction, four examples and a small guide to [mlr](https://github.com/mlr-org/mlr):

1. General introduction of the mechanics of the **hyperbandr** package
2. Example 1: hyperband to optimize a **neural network** with [mxnet](https://github.com/apache/incubator-mxnet/tree/master/R-package) and [mlr](https://github.com/mlr-org/mlr)
3. Example 2: hyperband in combination with MBO to optimize a **neural network with [mxnet](https://github.com/apache/incubator-mxnet/tree/master/R-package), [mlr](https://github.com/mlr-org/mlr) and [mlrMBO](https://github.com/mlr-org/mlrMBO)
4. Example 3: hyperband to optimize a **gradient boosting** model with [xgboost](https://github.com/dmlc/xgboost/tree/master/R-package) and [mlr](https://github.com/mlr-org/mlr)
5. Example 4: hyperband to optimize a **function** with [smoof](https://github.com/jakobbossek/smoof)
6. appendix: introduction to [mlr](https://github.com/mlr-org/mlr)

\newpage
## 1. General introduction

In order to call **hyperband**, we need to define five things:

1. a hyperparameter search space
2. a function to sample configurations
3. a function to initialize models
4. a function to train models
5. a function to evaluate the performance of a model

### 1: the hyperparameter search space

```{r, echo = TRUE, eval = FALSE}
mySearchSpace = ...
```

### 2: the sampling function

```{r, echo = TRUE, eval = FALSE}
sample.fun = function(par.set, n.configs, ...) {
  ...
}
```

The sampling function must return a list of named lists, containing the hyperparameter configurations. Consequently, the latter ones structure should look like this:

```{r, echo = FALSE}
library("ParamHelpers")

mySearchSpace = makeParamSet(
  makeDiscreteParam(id = "optimizer", values = c("sgd", "adam")),
  makeNumericParam(id = "learning.rate", lower = 0.001, upper = 0.1),
  makeLogicalParam(id = "batch.normalization")
)

sample.fun = function(par.set, n.configs, ...) {
  lapply(sampleValues(par = par.set, n = n.configs), function(x) x[!is.na(x)])
}

set.seed(6)

str(sample.fun(par.set = mySearchSpace, n.configs = 2))
```

### 3: the initialization function

```{r, echo = TRUE, eval = FALSE}
init.fun = function(r, config, problem) {
  ... 
}
```

### 4: the training function

```{r, echo = TRUE, eval = FALSE}
train.fun = function(mod, budget, problem) {
  ... 
}
```

### 5: the performance function

```{r, echo = TRUE, eval = FALSE}
performance.fun = function(model, problem) {
  ... 
}
```



```{r, echo = TRUE, eval = FALSE}
hyperhyper = hyperband(
  problem = problem,
  # these are the actual hyperband parameters
  max.ressources = 81, 
  prop.discard = 3,
  # since we chose accuracy, we want to maximize the performance
  max.perf = TRUE,
  id = "nnet", 
  par.set = configSpace, 
  sample.fun =  sample.fun,
  train.fun = train.fun, 
  performance.fun = performance.fun)
```


\newpage
## 2. Example 1: hyperband to optimize a **neural network** with [mxnet](https://github.com/apache/incubator-mxnet/tree/master/R-package) and [mlr](https://github.com/mlr-org/mlr)

```{r, echo = FALSE, warning = FALSE, message = FALSE}
#######################################
############## packages ###############
#######################################

library("R6")
library("devtools")
load_all()
library("mxnet")
library("mlr") # you might need to install mxnet branch of mlr: devtools::install_github("mlr-org/mlr", ref = "mxnet")
library("ggplot2")
library("gridExtra")
library("dplyr")
library("data.table")

####################################
## define the problem to optimize ##
####################################

# read mini_mnist (1/10 of actual mnist for faster evaluation, evenly distributed classes)
train = fread("../mnist/train.csv", header = TRUE)
test = fread("../mnist/test.csv", header = TRUE)

# Some operations to normalize features
mnist = as.data.frame(rbind(train, test))
mnist = mnist[sample(nrow(mnist)), ]
mnist[, 2:785] = lapply(mnist[, 2:785], function(x) x/255)

train.x = train[, -1]
train.x = t(train.x/255)
```

We would like to use a subset of the original MNIST data (LeCun & Cortes 2010) and tune a 
neural network with hyperbandr.

```{r, echo = FALSE}
plots = 64
visualize_this = sample(dim(train.x)[2], plots)
par(mfrow = c(8, plots/8), cex = 0.05)

for(i in 1:plots){
  train_vis = train.x[1:784, visualize_this[i]]
  train_mat = matrix(train_vis, nrow = 28, ncol = 28, byrow = TRUE)
  train_mat = apply(train_mat, 2 , rev)
  image(t(train_mat), axes = FALSE, col = grey(seq(from = 0, to = 1, length = 255)))
}

rm(train)
rm(train.x)
rm(test)
```

The data has 6000 observations, evenly distributed on 10 classes.

```{r, echo = TRUE}
dim(mnist)
table(mnist$label)
```

Let us create a list, which we call *problem*. 
That list should contain the data, as well as a subsampling rule. 

```{r, echo = TRUE}
# We sample 2/3 of our data for training
train.set = sample(nrow(mnist), size = (2/300)*nrow(mnist))

# Another 1/6 will be used for validation during training
val.set = sample(setdiff(1:nrow(mnist), train.set), 10)

# The remaining 1/6 will be stored for testing
test.set = setdiff(1:nrow(mnist), c(train.set, val.set))

# Since we use mlr, we define a classification task to encapsulate the data
task = makeClassifTask(data = mnist, target = "label")

# Finally, we define the problem list
problem = list(data = task, train = train.set, val = val.set, test = test.set)
```

```{r, echo = FALSE}
rm(mnist)
```

### 1: the configuration space

The ParamHelpers package provides an easy way to construct the configuration space

```{r, echo = TRUE, comment = NA}
library("ParamHelpers")
configSpace = makeParamSet(
  makeDiscreteParam(id = "optimizer", values = c("sgd", "rmsprop", "adam", "adagrad")),
  makeNumericParam(id = "learning.rate", lower = 0.001, upper = 0.1),
  makeNumericParam(id = "wd", lower = 0, upper = 0.01),
  makeNumericParam(id = "dropout.input", lower = 0, upper = 0.6),
  makeNumericParam(id = "dropout.layer1", lower = 0, upper = 0.6),
  makeNumericParam(id = "dropout.layer2", lower = 0, upper = 0.6),
  makeNumericParam(id = "dropout.layer3", lower = 0, upper = 0.6),
  makeLogicalParam(id = "batch.normalization1"),
  makeLogicalParam(id = "batch.normalization2"),
  makeLogicalParam(id = "batch.normalization3")
)
```

### 2: the sampling function

Now we need a function to sample configurations from our search space.

The input values of that function are:

* **par.set**: the search space
* **n.configs**: the number of configurations to sample
* **...**: additional arguments to access the bracket storage (see example 2 how to utilize this feature to combine hyperband with MBO)

```{r, echo = TRUE}
sample.fun = function(par.set, n.configs, ...) {
  # sample from the par.set and remove all NAs
  lapply(sampleValues(par = par.set, n = n.configs), function(x) x[!is.na(x)])
}
```

### 3: the initialization function

This function takes a config and samples a corresponding value of $x_2$ in order to initialize the model

```{r, echo = TRUE}
init.fun = function(r, config, problem) {
  lrn = makeLearner("classif.mxff",
    # you may have to install mxnet gpu, else just set ctx = mx.cpu()
    ctx = mx.gpu(),
    # we define a small CNN architecture with two conv and two dense layers
    # (the second dense layer is our and will be automatically created)
    layers = 3, 
    conv.layer1 = TRUE, conv.layer2 = TRUE,
    conv.data.shape = c(28, 28),
    num.layer1 = 8, num.layer2 = 16, num.layer3 = 32,
    conv.kernel1 = c(3,3), conv.stride1 = c(1,1), 
    pool.kernel1 = c(2,2), pool.stride1 = c(2,2),
    conv.kernel2 = c(3,3), conv.stride2 = c(1,1), 
    pool.kernel2 = c(2,2), pool.stride2 = c(2,2),           
    array.batch.size = 128,
    # we initialize our model with r iterations
    begin.round = 1, num.round = r,
    # here we allocate the configuration from our sample function
    par.vals = config
  )
  mod = train(learner = lrn, task = problem$data, subset = problem$train)
  return(mod)
}
```

### 4: the training function

To train our model, we simply sample values from a normal distribution and add or subtract them from our current $x_2$.
If the performance improves, we keep the model, else we discard it and keep the old one.

```{r, echo = TRUE}
train.fun = function(mod, budget, problem) {
  # we create a new learner and assign all parameters from our model
  lrn = makeLearner("classif.mxff", ctx = mx.gpu(), par.vals = mod$learner$par.vals)
  lrn = setHyperPars(lrn,
    # in addition, we have to extract the weights and feed them into our new model 
    symbol = mod$learner.model$symbol,
    arg.params = mod$learner.model$arg.params,
    aux.params = mod$learner.model$aux.params,
    begin.round = mod$learner$par.vals$begin.round + mod$learner$par.vals$num.round,
    num.round = budget
  )
  mod = train(learner = lrn, task = problem$data, subset = problem$train)
  return(mod)
}
```

### 5: the performance function

Finally, we define a function to evaluate the performance of each model

```{r, echo = TRUE}
performance.fun = function(model, problem) {
  pred = predict(model, task = problem$data, subset = problem$val)
  # we choose accuracy as our performance measure
  performance(pred, measures = acc)
}
```

## call hyperband

```{r, echo = TRUE, eval = FALSE}
hyperhyper = hyperband(
  problem = problem,
  # these are the actual hyperband parameters
  max.ressources = 81, 
  prop.discard = 3,
  # since we chose accuracy, we want to maximize the performance
  max.perf = TRUE,
  id = "nnet", 
  par.set = configSpace, 
  sample.fun =  sample.fun,
  train.fun = train.fun, 
  performance.fun = performance.fun)
```


\newpage
## 3. Example 2: hyperband in combination with MBO to optimize a **neural network** with [mxnet](https://github.com/apache/incubator-mxnet/tree/master/R-package), [mlr](https://github.com/mlr-org/mlr) and [mlrMBO](https://github.com/mlr-org/mlrMBO)



```{r, echo = TRUE, warning = FALSE}
library("mlrMBO")
library("ranger")

sample.fun.mbo = function(par.set, n.configs, bracket.storage) {
  # sample from configSpace
  if (dim(bracket.storage)[[1]] == 0) {
    lapply(sampleValues(par = par.set, n = n.configs), function(x) x[!is.na(x)])
  } else {
  # make MBO from bracket.storage  
    catf("Proposing points")
    ctrl = makeMBOControl(propose.points = n.configs)
    ctrl = setMBOControlInfill(ctrl, crit = crit.cb)
    designMBO = data.table(bracket.storage)
    designMBO = data.frame(designMBO[, max(y), by = names(configSpace$pars)])
    colnames(designMBO) = colnames(bracket.storage)[-(length(configSpace$pars) + 1)]
    opt.state = initSMBO(
      par.set = configSpace, 
      design = designMBO,
      control = ctrl, 
      minimize = FALSE, 
      noisy = FALSE)
    prop = proposePoints(opt.state)
    propPoints = prop$prop.points
    rownames(propPoints) = c()
    propPoints = convertRowsToList(propPoints, name.list = FALSE, name.vector = TRUE)
    return(propPoints)
  }
}
```



```{r, echo = TRUE}
hyperhyperMBO = hyperband(
  problem = problem, 
  max.ressources = 81, 
  prop.discard = 3,  
  max.perf = TRUE,
  id = "nnet", 
  par.set = configSpace, 
  sample.fun =  sample.fun.mbo,
  train.fun = train.fun, 
  performance.fun = performance.fun)
```



\newpage
## 4. Example 3: hyperband to optimize a **gradient boosting** model with [xgboost](https://github.com/dmlc/xgboost/tree/master/R-package) and [mlr](https://github.com/mlr-org/mlr)



```{r, echo = TRUE}
library("xgboost")


```



```{r, echo = TRUE}

```



\newpage
## 5. Example 4: hyperband to optimize a **function** with [smoof](https://github.com/jakobbossek/smoof)



```{r, echo = TRUE}

```



```{r, echo = TRUE}

```



\newpage
## 6. appendix: introduction to [mlr](https://github.com/mlr-org/mlr)



```{r, echo = TRUE}

```



```{r, echo = TRUE}

```






