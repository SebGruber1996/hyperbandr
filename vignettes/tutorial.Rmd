---
title: "hyperbandr tutorial"
author: "Niklas"
date: "23 MÃ¤rz 2018"
output: pdf_document
header-includes:
  - \usepackage{graphicx}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Hyperbandr Package

This is an R6 implementation of the original **hyperband** algorithm  <https://arxiv.org/abs/1603.06560>.

R6 is an encapsulated object oriented system akin to those in Java or C++, where objects contain methods in addition to data, and those methods can modify objects directly (unlike S3 and S4 which are both functional object-oriented systems, where class methods are separate from objects, and objects are not mutable).

Essentially, that means that we obtain a very generic implementation, which is working with every other R package (as long the algorithm meets the requirements of hyperband).

## This tutorial consists of three examples:

* hyperbandr to optimize the **branin function**
* hyperbandr to optimize **xgboost**
* hyperbandr to optimize a **neural network**

\newpage
## Example 1: hyperbandr to optimize the **branin function**

```{r, echo = TRUE, message = FALSE}
library("smoof")
library("data.table")
library("ggplot2")
```

```{r, echo = TRUE, fig.height = 3.6, fig.width = 6}
problem = makeBraninFunction()

# the branin function has 3 global minima (red dots)
opt = data.table(x1 = getGlobalOptimum(problem)$param$x1, 
                 x2 = getGlobalOptimum(problem)$param$x2)

(vis = autoplot(problem) + 
    geom_point(data = opt, aes(x = x1, y = x2), shape = 20, colour = "red", size = 5))
```


We treat the value of $x_1$ as our "configuration" and try to find the optimal value for $x_2$, our "hyperparameter" (reminder: in hyperband we sample random configurations in each bracket).

So in order to apply $\textbf{hyperbandr}$ on that problem, we need to define a hyperparameter space and four specific functions.

### Configuration Space

As our very first step, we need to define the hyperparameter space to sample our configurations from.
We want to obtain random values between -5 and approximately 10.1 (our $x_1$ axis..)

```{r, echo = TRUE}
# we use the makeParamSet function from the ParamHelpers package
configSpace = makeParamSet(
    makeNumericParam(id = "x1", lower = -5, upper = 10.1))
```

### Function 1: the sampling function

Now we need a function to sample configurations from the configuration space

```{r, echo = TRUE}
# par.set: the parameter space to sample from
# n.configs: the amount of configs to sample
# ...: additional parameters (see example ...........)
sample.fun = function(par.set, n.configs, ...) {
  sampleValues(par = par.set, n = n.configs)
}
```

### Function 2: the initialization function

This function takes a config and samples a corresponding value of $x_2$ in order to initialize the model

```{r, echo = TRUE}
# r: initial budget used for the initialization
# config: one configuration sampled by sample.fun
init.fun = function(r, config) {
  x1 = unname(unlist(config))
  x2 = runif(1, 0, 15)
  mod = c(x1, x2)
  return(mod)
}
```

### Function 3: the training function

To train our model, we simply sample values from a normal distribution and add or subtract them from our current $x_2$.
If the performance improves, we keep the model, else we discard it and keep the old one.

```{r, echo = TRUE}
# mod: the model to train
# budget: number of iterations to train the model for
train.fun = function(mod, budget) {
  for(i in seq_len(budget)) {
    mod.new = c(mod[[1]], mod[[2]] + rnorm(1, sd = 3))
    if(performance.fun(mod.new) < performance.fun(mod))
      mod = mod.new
  }
  return(mod)
}
```

### Function 4: the performance function

Finally, we define a function to evaluate the performance of each model

```{r, echo = TRUE}
# model: the model to evaluate
performance.fun = function(model) {
  problem(c(model[[1]], model[[2]]))
}
```

### Apply hyperbandr (since the problem to optimize here is very easy, the execution will only take 1-2 seconds)

```{r, echo = TRUE, message = FALSE}
library("R6")
library("devtools")
load_all()
```

```{r, echo = TRUE}
hyperhyper = hyperband(
  max.ressources = 81,
  prop.discard = 3,
  max.perf = FALSE,
  id = "branin",
  par.set = configSpace,
  sample.fun =  sample.fun,
  train.fun = train.fun,
  performance.fun = performance.fun)
```

\newpage
Let us inspect the results: we obtain a list of 5 R6 objects.

```{r, echo = TRUE}
hyperhyper
```

We could inspect the best configuration of bracket 3:

```{r, echo = TRUE}
hyperhyper[[3]]$models[[1]]$model
```

Or the first three configurations of bracket 2:

```{r, echo = TRUE}
hyperhyper[[2]]$configurations[1:3]
```

\newpage
Let us carry out a benchmark and repeat hyperband for 100 times to see if all brackets perform equally well.

```{r, echo = FALSE, message = FALSE, results = "hide"}
benchmarkThis = function(howManyIt = 10L) {
  results = data.frame(matrix(ncol = 5, nrow = howManyIt))
  for (i in 1:howManyIt) {
    catf("Iteration %i", i)
    hyperhyper = hyperband(
      max.perf = FALSE,
      max.ressources = 81,
      prop.discard = 3,
      id = "branin",
      par.set = configSpace,
      sample.fun =  sample.fun,
      train.fun = train.fun,
      performance.fun = performance.fun)
    results[i, 1] = round(hyperhyper[[1]]$getPerformances(), digits = 2)
    results[i, 2] = round(hyperhyper[[2]]$getPerformances(), digits = 2)
    results[i, 3] = round(hyperhyper[[3]]$getPerformances(), digits = 2)
    results[i, 4] = round(hyperhyper[[4]]$getPerformances(), digits = 2)
    results[i, 5] = round(hyperhyper[[5]]$getPerformances(), digits = 2)
  }
  return(results)
}

myBraninBenchmark = benchmarkThis(100)
```

```{r, echo = FALSE, message = FALSE}
ggplot(stack(myBraninBenchmark), aes(x = ind, y = values, fill = ind)) +
  scale_x_discrete(labels=c("bracket 1","bracket 2","bracket 3","bracket 4", "bracket 5")) +
  theme(legend.position = "none") + labs(x = "", y = "performance") +
  geom_boxplot()
```

Unsurprisingly, the fewer configurations are drawn, the worse the results become.