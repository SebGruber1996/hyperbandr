---
output: github_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  fig.path = "README-"
)
```


```{r, include = FALSE}
writeLines(capture.output(devtools::session_info()), "session_info.txt")
```


# hyperband in R6 

This is a very generic R6 implementation of the hyperband algorithm for hyperparameter optimization (https://arxiv.org/pdf/1603.06560.pdf)

The project is not yet finished but can already be used on your own problems and should work with any other R package/algorithm as long as it is suitable for hyperband.

### Please check the vignette folder for a very in-depth explanation + exhaustive examples on how to use the package and in particular, how to exploit the R6 class system in order to combine hyperband with MBO.


```{r, echo = FALSE, warning = FALSE, message = FALSE}
#######################################
############## packages ###############
#######################################

library("mlr3") 
library("ggplot2")
library("gridExtra")

####################################
## define the problem to optimize ##
####################################

# load german credit data (docs state custom cost function should be used,
# but is skipped here due to simplicity... maybe add later?)
task = mlr_tasks$get("german_data")

# create dummy encodings for factor variables
is_factor = which(sapply(german[-ncol(german)], is.factor))
german = createDummyFeatures(
  german, target = "label",
  cols = names(german[is_factor])
)
```

----

Let us use **hyperbandr** in order to tune the hyperparameters of a neural network on the famous MNIST data (LeCun & Cortes 2010).

To this, we use [mxnet](https://github.com/apache/incubator-mxnet/tree/master/R-package) and [mlr](https://github.com/mlr-org/mlr).

\vspace{10pt}

```{r, echo = FALSE}

#set.seed(123)

#plots = 64
#visualize_this = sample(dim(mnist)[2], plots)
#par(mfrow = c(8, plots/8), cex = 0.05)

#for(i in 1:plots){
#  train_vis = mnist[visualize_this[i], 1:784]
#  train_mat = matrix(as.numeric(train_vis), nrow = 28, ncol = 28, byrow = TRUE)
#  train_mat = apply(train_mat, 2 , rev)
#  image(t(train_mat), axes = FALSE, col = grey(seq(from = 0, to = 1, length = 255)))
#}

```

\vspace{10pt}

For convenience, we only use 1/10 of the original data.

\vspace{10pt}

```{r, echo = TRUE}
# We sample 2/3 of our data for training
train.set = sample(nrow(german), size = floor(2/3 * nrow(german)))

# Another 1/6 will be used for validation during training
val.set = sample(setdiff(1:nrow(german), train.set), size = floor(1/6 * nrow(german)))

# The remaining 1/6 will be stored for testing
test.set = setdiff(1:nrow(german), c(train.set, val.set))

# Since we use mlr, we define a classification task to encapsulate the data
task = makeClassifTask(data = german, target = "label")

# Finally, we define the problem list
problem = list(data = task, train = train.set, val = val.set, test = test.set)
```

\vspace{10pt}

At first we define a search space. 
The ParamHelpers package provides an easy way to construct the latter one.

\vspace{10pt}

```{r, echo = TRUE, comment = NA}

library(paradox)
tune_ps = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)
))


# replace with paradox
#library("ParamHelpers")
#configSpace = makeParamSet(
#  makeDiscreteParam(id = "booster",        values = c("gbtree", "gblinear", "dart")),
#  makeDiscreteParam(id = "sample_type",    values = c("uniform", "weighted")),
#  makeDiscreteParam(id = "normalize_type", values = c("tree", "forest")),
#  makeIntegerParam( id = "max_depth",              lower = 1, upper = 100),
#  makeIntegerParam( id = "num_parallel_tree",      lower = 1, upper = 100),
#  makeNumericParam( id = "eta",                    lower = 0, upper = 1),
#  makeNumericParam( id = "gamma",                  lower = 0, upper = 100),
#  makeNumericParam( id = "min_child_weight",       lower = 0, upper = 100),
#  makeNumericParam( id = "subsample",              lower = 0, upper = 1),
#  makeNumericParam( id = "colsample_bytree",       lower = 0, upper = 1),
#  makeNumericParam( id = "colsample_bylevel",      lower = 0, upper = 1),
#  makeNumericParam( id = "lambda",                 lower = 0, upper = 100),
#  makeNumericParam( id = "lambda_bias",            lower = 0, upper = 100),
#  makeNumericParam( id = "alpha",                  lower = 0, upper = 100),
#  makeNumericParam( id = "max_delta_step",         lower = 0, upper = 100),
#  makeNumericParam( id = "tweedie_variance_power", lower = 1, upper = 2),
#  makeNumericParam( id = "rate_drop",              lower = 0, upper = 1),
#  makeNumericParam( id = "skip_drop",              lower = 0, upper = 1)
#)

```

\vspace{10pt}

Now we need a function to sample configurations from our search space.

\vspace{10pt}

```{r, echo = TRUE}
sample.fun = function(par.set, n.configs, ...) {
  # sample from the par.set and remove all NAs
  lapply(sampleValues(par = par.set, n = n.configs), function(x) x[!is.na(x)])
}
```

\vspace{10pt}

.. as well as a function to initialize models ..

\vspace{10pt}

```{r, echo = TRUE}
init.fun = function(r, config, problem) {
  lrn = makeLearner("classif.xgboost",
    # we initialize our model with r iterations
    nrounds = r,
    # here we allocate the configuration from our sample function
    par.vals = config
  )
  mod = train(learner = lrn, task = problem$data, subset = problem$train)
  return(mod)
}
```

\vspace{10pt}

After each step of successive halving, hyperbandr continues training the remaining model instead of training from scratch. This will greatly speed training time. Thus, we need a function to continue the training of our models ..

We're planning to add training from scratch as well. That might be necessary if the architecture memory requirements become to big. 

\vspace{10pt}

```{r, echo = TRUE}
train.fun = function(mod, budget, problem) {
  # we create a new learner and assign all parameters from our model
  lrn = makeLearner("classif.xgboost", par.vals = mod$learner$par.vals)
#  lrn = setHyperPars(lrn,
#    # in addition, we have to extract the weights and feed them into our new model 
#    par.vals = mod$learner.model$par.vals,
#    nrounds = budget
#  )
  mod = train(learner = lrn, task = problem$data, subset = problem$train)
  return(mod)
}
```

\vspace{10pt}

.. and last but not least a function to measure the performance of our model at each step of successive halving:

\vspace{10pt}

```{r, echo = TRUE}
performance.fun = function(model, problem) {
  # predict the validation data
  pred = predict(model, task = problem$data, subset = problem$val)
  # we choose accuracy as our performance measure
  performance(pred, measures = acc)
}
```

\vspace{10pt}

Now we can call hyperband (with these hyperparameters, one run needs like _ minutes):

\vspace{10pt}

```{r, echo = TRUE, eval = TRUE, warning = FALSE}
library("hyperbandr")
hyperhyper = hyperband(
  problem = problem,
  max.resources = 81, 
  prop.discard = 3,
  max.perf = TRUE,
  id = "xgb", 
  par.set = configSpace, 
  sample.fun =  sample.fun,
  init.fun = init.fun,
  train.fun = train.fun, 
  performance.fun = performance.fun)
```

\vspace{10pt}

With max.resources = 81 and prop.discard = 3, we obtain a total of 5 brackets:

\vspace{10pt}

```{r, echo = TRUE, eval = TRUE}
length(hyperhyper)
```

\vspace{10pt}

We can inspect the first bracket ..

\vspace{10pt}

```{r, echo = TRUE, eval = TRUE}
hyperhyper[[1]]
```

\vspace{10pt}

.. and for instance check it's performance by calling the getPerformance() method:

\vspace{10pt}

```{r, echo = TRUE, eval = TRUE}
hyperhyper[[1]]$getPerformances()
```

\vspace{10pt}

We can also inspect the architecture of the best model of bracket 1:

\vspace{10pt}

```{r, echo = TRUE, eval = TRUE}
hyperhyper[[1]]$models[[1]]$model
```

\vspace{10pt}

Let's see which bracket yielded the best performance:

\vspace{10pt}

```{r, echo = TRUE, eval = TRUE}
lapply(hyperhyper, function(x) x$getPerformances())
```

\vspace{10pt}

We can call the hyperVis function to visualize all brackets:

\vspace{10pt}

```{r, echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
hyperVis(hyperhyper, perfLimits = c(0, 1))
```

\vspace{10pt}

Now we use the best model and predict test data:

\vspace{10pt}

```{r, echo = TRUE, eval = TRUE}
best.mod.index = which.max(unlist(lapply(hyperhyper, function(x) x$getPerformances())))
best.mod = hyperhyper[[best.mod.index]]$models[[1]]$model

performance(predict(object = best.mod, task = problem$data, subset = problem$test), 
            measures = acc)

```

\vspace{10pt}

### The example folder contains six detailed examples: 

* neural networks:
    + hyperband to tune hyperparameters with mxnet and mlr
    + combine hyperband and MBO to tunehyperparameters with mxnet, mlr and mlrMBO
* gradient boosting:
    + hyperband to tune hyperparameters with xgboost and mlr
    + combine hyperband and MBO to tunehyperparameters with xgboost, mlr and mlrMBO
* single- and multi-objective functions:
    + hyperband to tune hyperparameters with smoof and mlr
    + combine hyperband and MBO to tune hyperparameters with smoof, mlr and mlrMBO
    